{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from utils.helper_functions import * \n",
    "from utils.plotting_functions import * \n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statistics import mean,stdev \n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths to results for lineage level predictions for NA, EU and Asia\n",
    "\n",
    "results_CNT = \"North_America/results/Connecticut_results.json\"\n",
    "results_NE = \"Europe/results/Northern_Estonia_results.json\" \n",
    "results_MH = \"Asia/results/Maharashtra_results.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isdir(\"figures\") == False:\n",
    "    os.mkdir(\"figures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for plotting\n",
    "continents = [\"North_America\", \"Europe\", \"Asia\"]\n",
    "locations = [[\"Connecticut\", \"USA\", \"North_America\", \"Global\", \"Global_next_regions\"], [\"Estonia\", \"Europe\", \"Global\", \"Global_next_regions\"] , [\"India\", \"Asia\", \"Global\", \"Global_next_regions\"]]\n",
    "seeds = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]\n",
    "abundances = [1,2,3,4,5,6,7,8,9,10,20,30,40,50,60,70,80,90,100]\n",
    "results = [(\"North_America\", results_CNT), (\"Europe\",results_NE), (\"Asia\", results_MH)]\n",
    "\n",
    "all_results = dict()\n",
    "all_mean_results = dict()\n",
    "all_std_results = dict()\n",
    "\n",
    "for ((continent, results_path), locs) in zip(results, locations):\n",
    "    with open(results_path) as json_file:\n",
    "        res_file = json.loads(json_file.read())\n",
    "    # calculate absolute errors\n",
    "    absolute_errors = calculate_absolute_errors(res_file, seeds, abundances, locs)\n",
    "    # calculate  mean absolute errors per location & abundance\n",
    "    mean_abs_errors = dict()\n",
    "    stds = dict()\n",
    "    for loc in locs:\n",
    "        mean_abs_errors[loc] = dict()\n",
    "        stds[loc] = dict()\n",
    "        for ab in abundances:\n",
    "            mean_abs_errors[loc][ab] = round(mean(absolute_errors[loc][ab].values()),2)\n",
    "            stds[loc][ab] = round(stdev(absolute_errors[loc][ab].values()),2)\n",
    "\n",
    "    all_results[continent] = absolute_errors\n",
    "    all_std_results[continent] = stds\n",
    "    all_mean_results[continent] = mean_abs_errors\n",
    "\n",
    "# plot the mean absolute error per simulated abundance for each location in each continent\n",
    "locations_colors = [[(\"Connecticut\", \"#984ea3\") , (\"USA\", \"#d55e00\"), (\"North_America\", \"#dede00\") , (\"Global\", \"#0072b2\"), (\"Global_next_regions\", \"#009e73\")],\n",
    " [(\"Estonia\", \"#d55e00\"), (\"Europe\", \"#dede00\"), (\"Global\", \"#0072b2\"), (\"Global_next_regions\", \"#009e73\")] ,\n",
    "  [(\"India\", \"#d55e00\"), (\"Asia\", \"#dede00\"), (\"Global\", \"#0072b2\"), (\"Global_next_regions\", \"#009e73\")]]\n",
    "\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(10,5),sharex=True, sharey=True)\n",
    "fig.set_dpi(120)\n",
    "for continent, ax, locs in zip(continents, [ax1,ax2, ax3], locations_colors):\n",
    "    ax.grid()\n",
    "    ax.set_xticks([0,10,20,30,40,50,60,70,80,90,100])\n",
    "    # ax.set_yticks([0,10,20,30,40,50,60,70,80,90,100])\n",
    "    ax.set_title(continent.replace(\"_\", \" \"))\n",
    "    for (loc, col) in locs:\n",
    "        ax.plot(abundances, list(all_mean_results[continent][loc].values()), color = col) #, list(all_std_results[continent][loc].values()), color = col)\n",
    "        y = list(all_mean_results[continent][loc].values())\n",
    "        std = list(all_std_results[continent][loc].values())\n",
    "        addition = [sum(x) for x in zip(y, std)]\n",
    "        substraction = [yi - stdi for yi, stdi in zip(y, std)]\n",
    "        # comment out next line for non continuous error bars\n",
    "        ax.fill_between(abundances, substraction, addition, color= col, alpha=0.15, interpolate=True)\n",
    "        ax.set_yticks([0,10,20,30,40,50,60,70,80,90,100])\n",
    "        ax.grid(False)\n",
    "\n",
    "custom_lines = [Line2D([0], [0], color = \"#984ea3\", lw=1),\n",
    "                Line2D([0], [0], color = \"#d55e00\", lw=1),\n",
    "                Line2D([0], [0], color = \"#dede00\", lw=1),\n",
    "                Line2D([0], [0], color = \"#0072b2\", lw=1),\n",
    "                Line2D([0], [0], color = \"#009e73\", lw=1)]\n",
    "\n",
    "fig.legend(custom_lines,\n",
    " [\"State\\n(NA experiment only)\", \"Country\", \"Continent\", \"Global\", \"Global next regions\"],\n",
    "  loc=\"center right\",   # Position of legend\n",
    "  bbox_to_anchor=(1.12, 0.5)\n",
    ")\n",
    "\n",
    "fig.text(0.5, 0.015, 'Simulated abundance', va='center', ha='center', fontsize=13)\n",
    "fig.text(0.07, 0.5, 'Absolute error', va='center', ha='center', rotation='vertical', fontsize=13)\n",
    "fig.savefig(\"figures/lineplots.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot voc level predictions vs lineage level predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths to results for lineage level predictions and voc level predictions for the Connecticut experiment\n",
    "results_CNT_who = \"North_America/HPC/results/Connecticut_results_who.json\"\n",
    "results_CNT = \"North_America/HPC/results/Connecticut_results.json\"\n",
    "\n",
    "\n",
    "locations = [[\"Connecticut\", \"Global\"], [\"Connecticut\", \"Global\"]]\n",
    "seeds = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]\n",
    "abundances = [1,2,3,4,5,6,7,8,9,10,20,30,40,50,60,70,80,90,100]\n",
    "results = [(\"Lineage level predictions\", results_CNT), (\"VOC level predictions\", results_CNT_who)]\n",
    "\n",
    "all_results = dict()\n",
    "all_mean_results = dict()\n",
    "all_std_results = dict()\n",
    "\n",
    "for ((prediction_level, results_path), locs) in zip(results, locations):\n",
    "    with open(results_path) as json_file:\n",
    "        res_file = json.loads(json_file.read())\n",
    "    # calculate absolute errors\n",
    "    absolute_errors = calculate_absolute_errors(res_file, seeds, abundances, locs)\n",
    "    # calculate  mean absolute errors per location & abundance\n",
    "    mean_abs_errors = dict()\n",
    "    stds = dict()\n",
    "    for loc in locs:\n",
    "        mean_abs_errors[loc] = dict()\n",
    "        stds[loc] = dict()\n",
    "        for ab in abundances:\n",
    "            mean_abs_errors[loc][ab] = round(mean(absolute_errors[loc][ab].values()),2)\n",
    "            stds[loc][ab] = round(stdev(absolute_errors[loc][ab].values()),2)\n",
    "\n",
    "    all_results[prediction_level] = absolute_errors\n",
    "    all_std_results[prediction_level] = stds\n",
    "    all_mean_results[prediction_level] = mean_abs_errors\n",
    "\n",
    "# plot the mean absolute error per simulated abundance for each location in each continent\n",
    "\n",
    "prediction_level_colors = [[(\"Lineage level predictions\", \"#0072b2\") , (\"VOC level predictions\", \"#d55e00\")],\n",
    " [(\"Lineage level predictions\", \"#0072b2\") , (\"VOC level predictions\", \"#d55e00\")] ]\n",
    "\n",
    "prediction_levels = [\"Lineage level predictions\", \"VOC level predictions\"]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,5),sharex=True, sharey=True)\n",
    "fig.set_dpi(120)\n",
    "\n",
    "for (loc, ax), precol in zip([(\"Connecticut\", ax1), (\"Global\", ax2)], prediction_level_colors):\n",
    "    ax.grid()\n",
    "    ax.set_xticks([0,10,20,30,40,50,60,70,80,90,100])\n",
    "    ax.set_title(loc)\n",
    "    \n",
    "    for (pl, col) in precol:\n",
    "        # print(list(all_mean_results[pl][loc].values()), list(all_std_results[pl][loc].values()))\n",
    "        ax.plot(abundances, list(all_mean_results[pl][loc].values()) )#, list(all_std_results[pl][loc].values()), color = col) - for errorbar plot\n",
    "        y = list(all_mean_results[pl][loc].values())\n",
    "        std = list(all_std_results[pl][loc].values())\n",
    "        addition = [sum(x) for x in zip(y, std)]\n",
    "        substraction = [yi - stdi for yi, stdi in zip(y, std)]\n",
    "        # comment out next line for non continuous error bars\n",
    "        ax.fill_between(abundances, substraction, addition, color= col, alpha=0.2, interpolate=True)\n",
    "        ax.grid(False)\n",
    "        ax.set_yticks([0,10,20,30,40,50,60,70,80,90,100])\n",
    "\n",
    "custom_lines = [Line2D([0], [0], color = \"#0072b2\", lw=2),\n",
    "                Line2D([0], [0], color = \"#d55e00\", lw=2)]\n",
    "\n",
    "fig.legend(custom_lines,\n",
    "  prediction_levels,\n",
    "  loc=\"center right\",   # Position of legend\n",
    "  bbox_to_anchor=(1.15, 0.5)\n",
    ")\n",
    "\n",
    "fig.text(0.5, 0.015, 'Simulated abundance', va='center', ha='center', fontsize=13)\n",
    "fig.text(0.07, 0.5, 'Absolute error', va='center', ha='center', rotation='vertical', fontsize=13)\n",
    "fig.savefig(\"figures/lineplots_who.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Local reference set vs All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths to results for lineage level predictions and voc level predictions for the Connecticut experiment\n",
    "results_CNT_who = \"North_America/results/Connecticut_results_who.json\"\n",
    "results_CNT = \"North_America/results/Connecticut_results.json\"\n",
    "\n",
    "# paths to results for lineage level predictions and voc level predictions for the Estonian experiment\n",
    "results_EST_who = \"Europe/results/Northern_Estonia_results_who.json\"\n",
    "results_EST = \"Europe/results/Northern_Estonia_results.json\"\n",
    "\n",
    "# paths to results for lineage level predictions and voc level predictions for the Indian experiment\n",
    "results_IND_who = \"Asia/results/Maharashtra_results_who.json\"\n",
    "results_IND = \"Asia/results/Maharashtra_results.json\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continents_locations = [(\"North_America\", \"Connecticut\"), (\"Europe\", \"Estonia\"),  (\"Asia\", \"India\")]\n",
    "reference_sets = [[ \"USA\", \"North_America\", \"Global\", \"Global_next_regions\"], [ \"Europe\", \"Global\", \"Global_next_regions\"] , [ \"Asia\", \"Global\", \"Global_next_regions\"]]\n",
    "abundances = [1,2,3,4,5,6,7,8,9,10,20,30,40,50,60,70,80,90,100]\n",
    "results = [(\"North_America\", results_CNT), (\"Europe\",results_EST), (\"Asia\", results_IND)]\n",
    "results_who = [(\"North_America\", results_CNT_who), (\"Europe\",results_EST_who), (\"Asia\", results_IND_who)]\n",
    "seeds = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]\n",
    "abundances = [1,2,3,4,5,6,7,8,9,10,20,30,40,50,60,70,80,90,100]\n",
    "locations = [[\"Connecticut\", \"USA\", \"North_America\", \"Global\", \"Global_next_regions\"], [\"Estonia\", \"Europe\", \"Global\", \"Global_next_regions\"] , [\"India\", \"Asia\", \"Global\", \"Global_next_regions\"]]\n",
    "\n",
    "# lodad the results for lineage level predictions\n",
    "all_results = dict()\n",
    "\n",
    "for ((continent, results_path), locs) in zip(results, locations):\n",
    "    with open(results_path) as json_file:\n",
    "        res_file = json.loads(json_file.read())\n",
    "    # calculate absolute errors\n",
    "    absolute_errors = calculate_absolute_errors(res_file, seeds, abundances, locs)\n",
    "  \n",
    "    all_results[continent] = absolute_errors\n",
    "\n",
    "# lodad the results for voc level predictions\n",
    "all_results_who = dict()\n",
    "\n",
    "for ((continent, results_path), locs) in zip(results_who, locations):\n",
    "    with open(results_path) as json_file:\n",
    "        res_file = json.loads(json_file.read())\n",
    "    # calculate absolute errors\n",
    "    absolute_errors = calculate_absolute_errors(res_file, seeds, abundances, locs)\n",
    "  \n",
    "    all_results_who[continent] = absolute_errors\n",
    "\n",
    "p_values_voc = dict()\n",
    "p_values_lineage = dict()\n",
    "\n",
    "# calculate the p-values for lineage level predictions\n",
    "for ((continent, location), datasets) in zip(continents_locations, reference_sets):\n",
    "    p_values_lineage[location] = dict()\n",
    "    for dataset in datasets:\n",
    "        p_values_lineage[location][dataset] = dict()\n",
    "        for ab in abundances:\n",
    "            p_values_lineage[location][dataset][ab] = dict()\n",
    "            # values for the local dataset\n",
    "            local_dataset = all_results[continent][location][ab]\n",
    "            # values for the other dataset\n",
    "            other_dataset = all_results[continent][dataset][ab]\n",
    "            # calculate the p-value for the two-sided t-test\n",
    "            p_value_two_sided = stats.ttest_ind(list(local_dataset.values()), list(other_dataset.values()), equal_var=False)[1]\n",
    "            p_values_lineage[location][dataset][ab][\"two sided\"] = p_value_two_sided\n",
    "            #  calculate the p-value for the one-sided t-test\n",
    "            p_value_one_sided = stats.ttest_ind(list(local_dataset.values()), list(other_dataset.values()), equal_var=False, alternative='less')[1]\n",
    "            p_values_lineage[location][dataset][ab][\"one sided\"] = p_value_one_sided\n",
    "\n",
    "            \n",
    "# calculate the p-values for voc level predictions\n",
    "for ((continent, location), datasets) in zip(continents_locations, reference_sets):\n",
    "    p_values_voc[location] = dict()\n",
    "    for dataset in datasets:\n",
    "        p_values_voc[location][dataset] = dict()\n",
    "        for ab in abundances:\n",
    "            p_values_voc[location][dataset][ab] = dict()\n",
    "            # values for the local dataset\n",
    "            local_dataset = all_results_who[continent][location][ab]\n",
    "            # values for the other dataset\n",
    "            other_dataset = all_results_who[continent][dataset][ab]\n",
    "            # calculate the p-value for the two-sided t-test\n",
    "            p_value_two_sided = stats.ttest_ind(list(local_dataset.values()), list(other_dataset.values()), equal_var=False)[1]\n",
    "            p_values_voc[location][dataset][ab][\"two sided\"] = p_value_two_sided\n",
    "            #  calculate the p-value for the one-sided t-test\n",
    "            p_value_one_sided = stats.ttest_ind(list(local_dataset.values()), list(other_dataset.values()), equal_var=False, alternative='less')[1]\n",
    "            p_values_voc[location][dataset][ab][\"one sided\"] = p_value_one_sided\n",
    "\n",
    "# convert the results to dataframes\n",
    "for continent, location in continents_locations:\n",
    "    df_voc = pd.DataFrame.from_dict(p_values_voc[location])\n",
    "    df_lineage = pd.DataFrame.from_dict(p_values_lineage[location])\n",
    "    # save the results as csv files\n",
    "    df_voc.to_csv(\"statistical_analysis/\" + location + \"_p_values_voc.csv\")\n",
    "    df_lineage.to_csv(\"statistical_analysis/\" + location + \"_p_values_lineage.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot heatmap for voc level predictions and p-values for the two-sided t-test for the Connecticut experiment\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set(font_scale=1.7)\n",
    "# generate heatmaps for all experiments\n",
    "for continent, location in continents_locations:\n",
    "    df = pd.read_csv(\"statistical_analysis/\" + location + \"_p_values_voc.csv\" , index_col=0)\n",
    "    ar_2_sided = np.zeros((len(df.index), len(df.columns)))\n",
    "    ar_1_sided = np.zeros((len(df.index), len(df.columns)))\n",
    "    for i, indx1 in enumerate(df.index):\n",
    "        for j, indx2 in enumerate(df.columns):\n",
    "           ar_2_sided[i,j] = float(df.loc[indx1,indx2].split(\",\")[0].split(\":\")[1].replace(\"}\", \"\"))\n",
    "           ar_1_sided[i,j] = float(df.loc[indx1,indx2].split(\",\")[1].split(\":\")[1].replace(\"}\", \"\"))\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10,10))\n",
    "    cmap = cmap = sns.diverging_palette(220, 20, as_cmap=True)\n",
    "    sns.heatmap(ar_2_sided, cmap=cmap, ax=ax, xticklabels=[item.replace(\"_\", \" \") for item in df.columns], yticklabels=df.index, vmin=0, vmax = 0.1)\n",
    "    plt.xlabel(\"Dataset\")\n",
    "    plt.ylabel(\"Abundance\")\n",
    "    ax.set_title(location + \", voc level predictions, p-values for the two-sided t-test\")\n",
    "    plt.savefig(\"statistical_analysis/\" + location + \"_voc_level_predictions_p_values_two_sided_t_test.pdf\")\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10,10))\n",
    "    sns.heatmap(ar_1_sided, cmap=cmap, ax=ax, xticklabels=[item.replace(\"_\", \" \") for item in df.columns], yticklabels=df.index, vmin=0, vmax = 0.1)\n",
    "    ax.set_title(location + \", voc level predictions, p-values for the one-sided t-test\")\n",
    "    plt.xlabel(\"Dataset\")\n",
    "    plt.ylabel(\"Abundance\")\n",
    "    plt.savefig(\"statistical_analysis/\" + location + \"_voc_level_predictions_p_values_one_sided_t_test.pdf\")\n",
    "\n",
    "\n",
    "# do the same for lineage level predictions\n",
    "for continent, location in continents_locations:\n",
    "    df = pd.read_csv(\"statistical_analysis/\" + location + \"_p_values_lineage.csv\" , index_col=0)\n",
    "    ar_2_sided = np.zeros((len(df.index), len(df.columns)))\n",
    "    ar_1_sided = np.zeros((len(df.index), len(df.columns)))\n",
    "    for i, indx1 in enumerate(df.index):\n",
    "        for j, indx2 in enumerate(df.columns):\n",
    "           ar_2_sided[i,j] = float(df.loc[indx1,indx2].split(\",\")[0].split(\":\")[1].replace(\"}\", \"\"))\n",
    "           ar_1_sided[i,j] = float(df.loc[indx1,indx2].split(\",\")[1].split(\":\")[1].replace(\"}\", \"\"))\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10,10))\n",
    "    cmap = cmap = sns.diverging_palette(220, 20, as_cmap=True)\n",
    "    # make font bigger\n",
    "    sns.heatmap(ar_2_sided, cmap=cmap, ax=ax, xticklabels=[item.replace(\"_\", \" \") for item in df.columns], yticklabels=df.index, vmin=0, vmax = 0.1)\n",
    "    plt.xlabel(\"Dataset\")\n",
    "    plt.ylabel(\"Abundance\")\n",
    "    ax.set_title(location + \", lineage level predictions, p-values for the two-sided t-test\")\n",
    "    plt.savefig(\"statistical_analysis/\" + location + \"_lineage_level_predictions_p_values_two_sided_t_test.pdf\")\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10,10))\n",
    "    sns.heatmap(ar_1_sided, cmap=cmap, ax=ax, xticklabels=[item.replace(\"_\", \" \") for item in df.columns], yticklabels=df.index, vmin=0, vmax = 0.1)\n",
    "    plt.xlabel(\"Dataset\")\n",
    "    plt.ylabel(\"Abundance\")\n",
    "    # crete space between the title and the plot\n",
    "    ax.set_title(location + \", lineage level predictions, p-values for the one-sided t-test\")\n",
    "    \n",
    "    plt.savefig(\"statistical_analysis/\" + location + \"_lineage_level_predictions_p_values_one_sided_t_test.pdf\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All vs local reference set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "\n",
    "continents_locations = [(\"North_America\", \"Connecticut\"), (\"Europe\", \"Estonia\"),  (\"Asia\", \"India\")]\n",
    "reference_sets = [[ \"USA\", \"North_America\", \"Global\", \"Global_next_regions\"], [ \"Europe\", \"Global\", \"Global_next_regions\"] , [ \"Asia\", \"Global\", \"Global_next_regions\"]]\n",
    "abundances = [1,2,3,4,5,6,7,8,9,10,20,30,40,50,60,70,80,90,100]\n",
    "results = [(\"North_America\", results_CNT), (\"Europe\",results_EST), (\"Asia\", results_IND)]\n",
    "results_who = [(\"North_America\", results_CNT_who), (\"Europe\",results_EST_who), (\"Asia\", results_IND_who)]\n",
    "seeds = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]\n",
    "abundances = [1,2,3,4,5,6,7,8,9,10,20,30,40,50,60,70,80,90,100]\n",
    "locations = [[\"Connecticut\", \"USA\", \"North_America\", \"Global\", \"Global_next_regions\"], [\"Estonia\", \"Europe\", \"Global\", \"Global_next_regions\"] , [\"India\", \"Asia\", \"Global\", \"Global_next_regions\"]]\n",
    "\n",
    "# lodad the results for lineage level predictions\n",
    "all_results = dict()\n",
    "\n",
    "for ((continent, results_path), locs) in zip(results, locations):\n",
    "    with open(results_path) as json_file:\n",
    "        res_file = json.loads(json_file.read())\n",
    "    # calculate absolute errors\n",
    "    absolute_errors = calculate_absolute_errors(res_file, seeds, abundances, locs)\n",
    "  \n",
    "    all_results[continent] = absolute_errors\n",
    "\n",
    "# lodad the results for voc level predictions\n",
    "all_results_who = dict()\n",
    "\n",
    "for ((continent, results_path), locs) in zip(results_who, locations):\n",
    "    with open(results_path) as json_file:\n",
    "        res_file = json.loads(json_file.read())\n",
    "    # calculate absolute errors\n",
    "    absolute_errors = calculate_absolute_errors(res_file, seeds, abundances, locs)\n",
    "  \n",
    "    all_results_who[continent] = absolute_errors\n",
    "\n",
    "p_values_voc = dict()\n",
    "p_values_lineage = dict()\n",
    "\n",
    "# calculate the p-values for lineage level predictions\n",
    "for ((continent, location), datasets) in zip(continents_locations, reference_sets):\n",
    "    p_values_lineage[location] = dict()\n",
    "    for dataset in datasets:\n",
    "        p_values_lineage[location][dataset] = dict()\n",
    "        for ab in abundances:\n",
    "            # values for the local dataset\n",
    "            local_dataset = all_results[continent][location][ab]\n",
    "            # values for the other dataset\n",
    "            other_dataset = all_results[continent][dataset][ab]\n",
    "            #  calculate the p-value for the one-sided t-test\n",
    "            p_value_one_sided = stats.ttest_ind(list(other_dataset.values()), list(local_dataset.values()), equal_var=False, alternative='less')[1]\n",
    "            p_values_lineage[location][dataset][ab] = p_value_one_sided\n",
    "\n",
    "            \n",
    "# calculate the p-values for voc level predictions\n",
    "for ((continent, location), datasets) in zip(continents_locations, reference_sets):\n",
    "    p_values_voc[location] = dict()\n",
    "    for dataset in datasets:\n",
    "        p_values_voc[location][dataset] = dict()\n",
    "        for ab in abundances:\n",
    "            # values for the local dataset\n",
    "            local_dataset = all_results_who[continent][location][ab]\n",
    "            # values for the other dataset\n",
    "            other_dataset = all_results_who[continent][dataset][ab]\n",
    "            #  calculate the p-value for the one-sided t-test\n",
    "            p_value_one_sided = stats.ttest_ind(list(other_dataset.values()), list(local_dataset.values()), equal_var=False, alternative='less')[1]\n",
    "            p_values_voc[location][dataset][ab] = p_value_one_sided\n",
    "\n",
    "# convert the results to dataframes\n",
    "for continent, location in continents_locations:\n",
    "    df_voc = pd.DataFrame.from_dict(p_values_voc[location])\n",
    "    df_lineage = pd.DataFrame.from_dict(p_values_lineage[location])\n",
    "    # save the results as csv files\n",
    "    df_voc.to_csv(\"statistical_analysis/all_vs_local/\" + location + \"_p_values_voc.csv\")\n",
    "    df_lineage.to_csv(\"statistical_analysis/all_vs_local/\" + location + \"_p_values_lineage.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot heatmap for voc level predictions and p-values for the two-sided t-test for the Connecticut experiment\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# generate heatmaps for all experiments\n",
    "# voc level predictions\n",
    "for continent, location in continents_locations:\n",
    "    df = pd.read_csv(\"statistical_analysis/all_vs_local/\" + location + \"_p_values_voc.csv\" , index_col=0)\n",
    "    ar_1_sided = np.zeros((len(df.index), len(df.columns)))\n",
    "    for i, indx1 in enumerate(df.index):\n",
    "        for j, indx2 in enumerate(df.columns):\n",
    "           ar_1_sided[i,j] = float(df.loc[indx1,indx2])\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10,10))\n",
    "    sns.heatmap(ar_1_sided, cmap=cmap, ax=ax, xticklabels=[item.replace(\"_\", \" \") for item in df.columns], yticklabels=df.index, vmin=0, vmax = 0.1)\n",
    "    plt.xlabel(\"Dataset\")\n",
    "    plt.ylabel(\"Abundance\")\n",
    "    ax.set_title(location + \", voc level predictions, p-values for the one-sided t-test\")\n",
    "    plt.savefig(\"statistical_analysis/all_vs_local/\" + location + \"_voc_level_predictions_p_values_one_sided_t_test.pdf\")\n",
    "\n",
    "\n",
    "# do the same for lineage level predictions\n",
    "for continent, location in continents_locations:\n",
    "    df = pd.read_csv(\"statistical_analysis/all_vs_local/\" + location + \"_p_values_lineage.csv\" , index_col=0)\n",
    "    ar_1_sided = np.zeros((len(df.index), len(df.columns)))\n",
    "    for i, indx1 in enumerate(df.index):\n",
    "        for j, indx2 in enumerate(df.columns):\n",
    "           ar_1_sided[i,j] = float(df.loc[indx1,indx2])\n",
    "           \n",
    "    fig, ax = plt.subplots(figsize=(10,10))\n",
    "    # set font size for everything to 20\n",
    "    sns.heatmap(ar_1_sided, cmap=cmap, ax=ax, xticklabels=[item.replace(\"_\", \" \") for item in df.columns], yticklabels=df.index, vmin=0, vmax = 0.1)\n",
    "    ax.set_title(location + \", lineage level predictions, p-values for the one-sided t-test\")\n",
    "    plt.xlabel(\"Dataset\")\n",
    "    plt.ylabel(\"Abundance\")\n",
    "    plt.savefig(\"statistical_analysis/all_vs_local/\" + location + \"_lineage_level_predictions_p_values_one_sided_t_test.pdf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
